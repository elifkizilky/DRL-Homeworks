{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fca610e-3b7d-4b83-aaa4-94b0c701d4fc",
   "metadata": {},
   "source": [
    "REFERENCES:\n",
    "https://github.com/TychoTheTaco/Car-Racing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddbd4ea9-182f-45c7-8eae-de0cf66bf50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Below code does not work well for task 3 but works well for task 4 (ppo)\n",
    "'''\n",
    "#Importing necessary packages\n",
    "#This environment is a slightly modified version of CarRacing-v0\n",
    "from environments.custom_car_racing import CustomCarRacing\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow_probability.python.distributions.beta import Beta\n",
    "from tensorflow.keras import layers\n",
    "import gym\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Union, Optional\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# IF TRAINING BY PPO IS PREFERRED:\n",
    "ppo_train = True\n",
    "\n",
    "# Disable all GPUs\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "visible_devices = tf.config.get_visible_devices()\n",
    "for device in visible_devices:\n",
    "    assert device.device_type != 'GPU'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49a9a60-1b8d-4859-9d8a-3c6ce4f4c23e",
   "metadata": {},
   "source": [
    "TASK 1-2-3-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f70d76c-d2b1-4d3d-915d-b32c01c92d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing network\n",
    "class Network():\n",
    "    def __init__(self, env: gym.Env):\n",
    "        self._env = env\n",
    "        self._model = self._create_model()\n",
    "        self._model.summary()\n",
    "     \n",
    "    #Creating the Model    \n",
    "    def _create_model(self):\n",
    "        def create_conv_layer(filters, kernel_size, strides):\n",
    "            return layers.Conv2D(filters, kernel_size=kernel_size, strides=strides, activation='relu', kernel_initializer=tf.initializers.glorot_normal(),\n",
    "                                 bias_initializer=tf.initializers.constant(0.1))\n",
    "\n",
    "        # Input is a stack of frames\n",
    "        input_0 = layers.Input(shape=(32, 32, 4))\n",
    "\n",
    "        # Main network backbone. This is shared by the actor and critic.\n",
    "        conv_0 = create_conv_layer(8, 4, 2)(input_0)\n",
    "        conv_1 = create_conv_layer(16, 3, 2)(conv_0)\n",
    "        conv_2 = create_conv_layer(32, 3, 2)(conv_1)\n",
    "        flat_0 = layers.Flatten()(conv_2)\n",
    "\n",
    "        # Actor output\n",
    "        dense_0 = layers.Dense(64, activation='relu')(flat_0)\n",
    "        dense_1 = layers.Dense(6, activation='softplus')(dense_0)\n",
    "        reshape_0 = layers.Reshape((3, 2))(dense_1)\n",
    "        lamb_0 = layers.Lambda(lambda x: x + 1)(reshape_0)  # Ensure alpha and beta are > 1\n",
    "\n",
    "        # Critic output\n",
    "        dense_2 = layers.Dense(64, activation='relu')(flat_0)\n",
    "        dense_3 = layers.Dense(1)(dense_2)\n",
    "\n",
    "        # Compile model\n",
    "        model = tf.keras.Model(inputs=[input_0], outputs=[lamb_0, dense_3])\n",
    "        model.compile(optimizer=tf.optimizers.Adam(0.001))\n",
    "\n",
    "        return model\n",
    "    \n",
    "    #Given a (batch of) state(s), sample a (batch of) action(s), return the action(s) and the respective log-probability(ies)\n",
    "    def sample_actions(self,state):\n",
    "        p = self._model(np.expand_dims(state, axis=0))[0][0]\n",
    "        alpha, beta = p[:, 0], p[:, 1]\n",
    "        beta_distribution = Beta(alpha, beta)\n",
    "        action = beta_distribution.sample()\n",
    "        log_prob = tf.reduce_sum(beta_distribution.log_prob(action))\n",
    "        return action, log_prob\n",
    "\n",
    "    \n",
    "    \n",
    "    #Given a (batch of) state(s) and action(s), return the (batch of) log probability(ies) of the action(s) given the state(s)\n",
    "    def sample_log(self, states, actions):\n",
    "        log_probs = []\n",
    "        for state in states:\n",
    "            acts, log_prob = sample_actions(state)\n",
    "            for i in range(len(acts)):\n",
    "                for action in actions:\n",
    "                    if act == action:\n",
    "                        log_probs.append((action,state, log_prob[i]))\n",
    "        return log_probs\n",
    "        \n",
    "    #Returns an action\n",
    "    def get_action(self, observation, action_space: gym.Space):\n",
    "        p = self._model.predict(np.expand_dims(observation, axis=0))[0][0]\n",
    "        alpha, beta = p[:, 0], p[:, 1]\n",
    "        distribution = Beta(alpha, beta)\n",
    "        action = distribution.sample().numpy()\n",
    "        action[0] = np.interp(action[0], [0, 1], [-1, 1])\n",
    "        return action\n",
    "    \n",
    "    #Training for ppo (if ppo==True) or vanilla policy gradient (if ppo==false)\n",
    "    def train(self,\n",
    "              render = False,\n",
    "              ppo = False,\n",
    "              episodes: int = 1000,\n",
    "              log_interval: int = 10,\n",
    "              model_dir: str = 'models',\n",
    "              save_interval: int = 100,\n",
    "              buffer_size: int = 2000,\n",
    "              trajectory_size: int = 128,\n",
    "              gamma: float = 0.99,\n",
    "              ppo_epochs: int = 10,\n",
    "              clip_epsilon: float = 0.1):\n",
    "        model_dir = Path(model_dir, datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\n",
    "        model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        training_start_time = datetime.datetime.now()\n",
    "        print('Started training at', training_start_time.strftime('%d-%m-%Y %H:%M:%S'))\n",
    "\n",
    "        # Keep track of some stats\n",
    "        episode_rewards = []\n",
    "        moving_average_range = 50\n",
    "\n",
    "        transitions = []\n",
    "\n",
    "        for episode in range(1, episodes + 1):\n",
    "\n",
    "            # Reset environment\n",
    "            observation = self._env.reset()\n",
    "            episode_reward = 0\n",
    "\n",
    "            done = False\n",
    "            while not done:\n",
    "                if render: self._env.render()\n",
    "                # Choose action\n",
    "                p = self._model(np.expand_dims(observation, axis=0))[0][0]\n",
    "                alpha, beta = p[:, 0], p[:, 1]\n",
    "                beta_distribution = Beta(alpha, beta)\n",
    "                action = beta_distribution.sample()\n",
    "                log_prob = tf.reduce_sum(beta_distribution.log_prob(action))\n",
    "\n",
    "                A = action.numpy()\n",
    "                A[0] = np.interp(A[0], [0., 1.], [-1., 1.])\n",
    "                #print(f'Action: {A}')\n",
    "\n",
    "                # Perform action\n",
    "                new_observation, reward, done, _ = self._env.step(A)\n",
    "                episode_reward += reward\n",
    "\n",
    "                transitions.append((observation, action, log_prob, reward, new_observation))\n",
    "                if len(transitions) >= buffer_size:\n",
    "                    print('learning!')\n",
    "\n",
    "                    states = tf.convert_to_tensor([x[0] for x in transitions])\n",
    "                    actions = tf.convert_to_tensor([x[1] for x in transitions])\n",
    "                    old_a_logp = tf.expand_dims(tf.convert_to_tensor([x[2] for x in transitions]), axis=1)\n",
    "                    rewards = tf.expand_dims(tf.convert_to_tensor([x[3] for x in transitions], dtype=np.float32), axis=1)\n",
    "                    new_states = tf.convert_to_tensor([x[4] for x in transitions])\n",
    "\n",
    "                    discounted_rewards = rewards + gamma * self._model(new_states)[1]\n",
    "                    adv = discounted_rewards - self._model(states)[1]\n",
    "\n",
    "                    \n",
    "                    '''\n",
    "                        TASK 2\n",
    "                        This function helps to create a trajectory\n",
    "                    '''\n",
    "                    def gen_trajectory(indices, trajectory_size):\n",
    "                        for i in range(0, len(indices), trajectory_size):\n",
    "                            yield indices[i:i + trajectory_size]\n",
    "\n",
    "                    for _ in range(ppo_epochs):\n",
    "                        indices = np.arange(buffer_size)\n",
    "                        np.random.shuffle(indices)\n",
    "\n",
    "                        for batch in gen_trajectory(indices, trajectory_size):\n",
    "\n",
    "                            with tf.GradientTape() as tape:\n",
    "\n",
    "                                # Calculate action loss\n",
    "                                ab = self._model(tf.gather(states, batch))[0]\n",
    "                                alpha, beta = ab[:, :, 0], ab[:, :, 1]\n",
    "                                dist = Beta(alpha, beta)\n",
    "                                a_logp = tf.reduce_sum(dist.log_prob(tf.gather(actions, batch)), axis=1, keepdims=True)\n",
    "                                \n",
    "                                '''\n",
    "                                    TASK 4 & 3\n",
    "                                '''\n",
    "                                if ppo:\n",
    "                                    ratio = tf.exp(a_logp - tf.gather(old_a_logp, batch))\n",
    "                                    surr1 = ratio * tf.gather(adv, batch)\n",
    "                                    surr2 = tf.clip_by_value(ratio, 1.0 - clip_epsilon, 1.0 + clip_epsilon) * tf.gather(adv, batch)\n",
    "                                    action_loss = tf.reduce_mean(-tf.minimum(surr1, surr2))\n",
    "                                    #print(action_loss)\n",
    "                                else:\n",
    "                                    surr = a_logp * tf.gather(adv, batch)\n",
    "                                    action_loss = tf.reduce_mean(-surr)\n",
    "                                    #print(action_loss)\n",
    "                                    '''\n",
    "                                    probability = tf.reduce_sum(dist.prob(tf.gather(actions, batch)), axis=1, keepdims=True)      \n",
    "                                    p_loss= []\n",
    "                                    e_loss = []\n",
    "                                    td = tf.gather(adv,batch)\n",
    "                                    td = td.numpy()\n",
    "                                    #print(td)\n",
    "                                    for pb, t, lpb in zip(probability, td, a_logp):\n",
    "                                                    t =  tf.constant(t)\n",
    "                                                    policy_loss = tf.math.multiply(lpb,t)\n",
    "                                                    entropy_loss = tf.math.negative(tf.math.multiply(pb,lpb))\n",
    "                                                    p_loss.append(policy_loss)\n",
    "                                                    e_loss.append(entropy_loss)\n",
    "                                    p_loss = tf.stack(p_loss)\n",
    "                                    e_loss = tf.stack(e_loss)\n",
    "                                    p_loss = tf.reduce_mean(p_loss)\n",
    "                                    e_loss = tf.reduce_mean(e_loss)\n",
    "                                    # print(p_loss)\n",
    "                                    # print(e_loss)\n",
    "                                    action_loss = -p_loss - 0.0001 * e_loss\n",
    "                                    #print(loss)\n",
    "                                   '''\n",
    "                                    \n",
    "                            \n",
    "                                # Calculate value loss\n",
    "                                value_loss = tf.reduce_mean(\n",
    "                                    tf.losses.mse(\n",
    "                                        tf.gather(discounted_rewards, batch),\n",
    "                                        self._model(tf.gather(states, batch))[1]\n",
    "                                    ))\n",
    "                                \n",
    "\n",
    "                                # Calculate combined loss\n",
    "                                loss = action_loss + 2 * value_loss\n",
    "\n",
    "                            g = tape.gradient(loss, self._model.trainable_variables)\n",
    "                            self._model.optimizer.apply_gradients(zip(g, self._model.trainable_variables))\n",
    "\n",
    "                    transitions.clear()\n",
    "\n",
    "                observation = new_observation\n",
    "\n",
    "            episode_rewards.append(episode_reward)\n",
    "\n",
    "            # Print some statistics\n",
    "            if not episode % log_interval:\n",
    "                print(f'Episode {episode} | Reward: {episode_reward:.02f} | Moving Average: {np.average(episode_rewards[-50:]):.02f}')\n",
    "\n",
    "            # Save model\n",
    "            if not episode % save_interval:\n",
    "                self._model.save(model_dir / f'episode-{episode}.h5')\n",
    "\n",
    "        # Save final model\n",
    "        self._model.save(model_dir / 'model.h5')\n",
    "\n",
    "        training_end_time = datetime.datetime.now()\n",
    "        print('Finished training at', training_end_time.strftime('%d-%m-%Y %H:%M:%S'))\n",
    "        print('Total training time:', training_end_time - training_start_time)\n",
    "        np.savetxt(model_dir / 'rewards.txt', episode_rewards)\n",
    "\n",
    "        # Plot statistics\n",
    "        x_axis = np.arange(len(episode_rewards))\n",
    "        plt.figure(1, figsize=(16, 9))\n",
    "        plt.plot(x_axis, episode_rewards, label='Episode reward')\n",
    "        moving_averages = [np.mean(episode_rewards[i - (moving_average_range - 1):i + 1]) if i >= (moving_average_range - 1) else np.mean(episode_rewards[:i + 1]) for i in range(len(episode_rewards))]\n",
    "        plt.plot(x_axis, moving_averages, color='red', label=f'{moving_average_range}-episode moving average')\n",
    "        plt.title('Training Performance')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Score')\n",
    "        plt.legend(loc='upper left')\n",
    "        plt.savefig(model_dir / 'rewards.jpg')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea115a5-4db7-4292-90c3-b3b1ba8c6b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 32, 32, 4)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 15, 15, 8)    520         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 7, 7, 16)     1168        ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 3, 3, 32)     4640        ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 288)          0           ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 64)           18496       ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 6)            390         ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 3, 2)         0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 64)           18496       ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 3, 2)         0           ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 1)            65          ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 43,775\n",
      "Trainable params: 43,775\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Started training at 10-07-2022 14:49:51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\elifk\\anaconda3\\lib\\site-packages\\pyglet\\image\\codecs\\wic.py:406: UserWarning: [WinError -2147417850] İş parçacığı modu kurulduktan sonra değiştirilemez\n",
      "  warnings.warn(str(err))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10 | Reward: -10.56 | Moving Average: -24.58\n",
      "Episode 20 | Reward: -16.94 | Moving Average: -26.95\n",
      "learning!\n",
      "Episode 30 | Reward: -27.61 | Moving Average: -27.83\n",
      "Episode 40 | Reward: -18.06 | Moving Average: -27.75\n",
      "learning!\n",
      "Episode 50 | Reward: -33.68 | Moving Average: -28.03\n",
      "Episode 60 | Reward: -63.19 | Moving Average: -26.99\n",
      "learning!\n",
      "Episode 70 | Reward: -28.31 | Moving Average: -25.99\n",
      "Episode 80 | Reward: -12.11 | Moving Average: -24.24\n",
      "learning!\n",
      "Episode 90 | Reward: -29.98 | Moving Average: -21.78\n",
      "Episode 100 | Reward: -18.53 | Moving Average: -19.77\n",
      "learning!\n",
      "Episode 110 | Reward: -10.51 | Moving Average: -18.85\n",
      "Episode 120 | Reward: 3.60 | Moving Average: -16.44\n",
      "learning!\n",
      "Episode 130 | Reward: -6.28 | Moving Average: -15.03\n",
      "Episode 140 | Reward: 1.40 | Moving Average: -13.84\n",
      "Episode 150 | Reward: -8.12 | Moving Average: -11.16\n",
      "Episode 160 | Reward: -11.96 | Moving Average: -9.79\n",
      "learning!\n",
      "Episode 170 | Reward: -13.87 | Moving Average: -8.96\n",
      "Episode 180 | Reward: -9.71 | Moving Average: -8.35\n",
      "Episode 190 | Reward: -12.44 | Moving Average: -6.77\n",
      "learning!\n",
      "Episode 200 | Reward: -9.22 | Moving Average: -6.81\n",
      "Episode 210 | Reward: -8.16 | Moving Average: -6.33\n",
      "Episode 220 | Reward: 0.49 | Moving Average: -4.57\n",
      "learning!\n",
      "Episode 230 | Reward: 1.27 | Moving Average: -2.58\n",
      "Episode 240 | Reward: 10.35 | Moving Average: -2.21\n",
      "Episode 250 | Reward: -5.73 | Moving Average: -1.35\n",
      "Episode 260 | Reward: -11.12 | Moving Average: 0.12\n",
      "learning!\n",
      "Episode 270 | Reward: 7.54 | Moving Average: 0.47\n",
      "Episode 280 | Reward: 13.06 | Moving Average: 1.14\n",
      "Episode 290 | Reward: -11.36 | Moving Average: 0.17\n",
      "learning!\n",
      "Episode 300 | Reward: -3.97 | Moving Average: 1.53\n",
      "Episode 310 | Reward: 0.20 | Moving Average: 0.47\n",
      "Episode 320 | Reward: -3.68 | Moving Average: 0.16\n",
      "Episode 330 | Reward: 7.00 | Moving Average: 0.30\n",
      "Episode 340 | Reward: 2.20 | Moving Average: 1.40\n",
      "learning!\n",
      "Episode 350 | Reward: 1.44 | Moving Average: 1.67\n",
      "Episode 360 | Reward: 3.19 | Moving Average: 2.88\n",
      "Episode 370 | Reward: -5.64 | Moving Average: 3.44\n",
      "Episode 380 | Reward: -3.91 | Moving Average: 2.96\n",
      "Episode 390 | Reward: -1.45 | Moving Average: 3.09\n",
      "learning!\n",
      "Episode 400 | Reward: 8.13 | Moving Average: 3.31\n",
      "Episode 410 | Reward: 7.04 | Moving Average: 3.94\n",
      "Episode 420 | Reward: 0.88 | Moving Average: 3.57\n",
      "Episode 430 | Reward: 3.16 | Moving Average: 4.11\n",
      "Episode 440 | Reward: -1.12 | Moving Average: 3.98\n",
      "learning!\n",
      "Episode 450 | Reward: 0.70 | Moving Average: 3.08\n",
      "Episode 460 | Reward: 1.48 | Moving Average: 3.12\n",
      "Episode 470 | Reward: -3.50 | Moving Average: 3.26\n",
      "Episode 480 | Reward: -8.21 | Moving Average: 3.03\n",
      "Episode 490 | Reward: -2.47 | Moving Average: 2.95\n",
      "learning!\n",
      "Episode 500 | Reward: -3.50 | Moving Average: 3.52\n",
      "Episode 510 | Reward: -2.31 | Moving Average: 2.69\n",
      "Episode 520 | Reward: 13.49 | Moving Average: 3.92\n",
      "Episode 530 | Reward: -6.83 | Moving Average: 4.56\n",
      "learning!\n",
      "Episode 540 | Reward: 3.52 | Moving Average: 5.09\n",
      "Episode 550 | Reward: 1.29 | Moving Average: 6.31\n",
      "Episode 560 | Reward: 1.68 | Moving Average: 6.67\n",
      "Episode 570 | Reward: 12.72 | Moving Average: 6.00\n",
      "Episode 580 | Reward: -2.03 | Moving Average: 7.41\n",
      "learning!\n",
      "Episode 590 | Reward: 3.25 | Moving Average: 10.56\n",
      "Episode 600 | Reward: -2.63 | Moving Average: 9.50\n",
      "Episode 610 | Reward: -3.83 | Moving Average: 9.99\n",
      "Episode 620 | Reward: -1.94 | Moving Average: 10.68\n",
      "learning!\n",
      "Episode 630 | Reward: 1.85 | Moving Average: 12.44\n",
      "Episode 640 | Reward: -0.43 | Moving Average: 11.34\n",
      "Episode 650 | Reward: -8.77 | Moving Average: 13.77\n",
      "Episode 660 | Reward: 4.18 | Moving Average: 16.12\n",
      "learning!\n",
      "Episode 670 | Reward: 10.82 | Moving Average: 18.46\n",
      "Episode 680 | Reward: 18.68 | Moving Average: 17.63\n",
      "Episode 690 | Reward: 144.69 | Moving Average: 21.79\n",
      "learning!\n",
      "Episode 700 | Reward: 2.76 | Moving Average: 20.60\n",
      "Episode 710 | Reward: 31.51 | Moving Average: 23.10\n",
      "Episode 720 | Reward: 27.87 | Moving Average: 24.58\n",
      "Episode 730 | Reward: 6.80 | Moving Average: 26.38\n",
      "learning!\n",
      "Episode 740 | Reward: -5.63 | Moving Average: 27.13\n",
      "Episode 750 | Reward: 140.33 | Moving Average: 34.43\n",
      "learning!\n",
      "Episode 760 | Reward: 38.54 | Moving Average: 36.42\n",
      "Episode 770 | Reward: 34.57 | Moving Average: 39.95\n",
      "Episode 780 | Reward: 32.70 | Moving Average: 45.05\n",
      "learning!\n",
      "Episode 790 | Reward: 457.74 | Moving Average: 56.60\n",
      "Episode 800 | Reward: 52.71 | Moving Average: 66.97\n",
      "learning!\n",
      "Episode 810 | Reward: 71.79 | Moving Average: 75.59\n",
      "Episode 820 | Reward: 58.08 | Moving Average: 83.24\n",
      "learning!\n",
      "Episode 830 | Reward: 333.36 | Moving Average: 95.38\n",
      "Episode 840 | Reward: 12.03 | Moving Average: 107.23\n",
      "learning!\n",
      "Episode 850 | Reward: 366.16 | Moving Average: 120.90\n",
      "learning!\n",
      "Episode 860 | Reward: 41.40 | Moving Average: 128.89\n",
      "Episode 870 | Reward: 112.16 | Moving Average: 138.10\n",
      "learning!\n",
      "Episode 880 | Reward: 314.46 | Moving Average: 172.78\n",
      "learning!\n",
      "Episode 890 | Reward: 242.24 | Moving Average: 193.33\n",
      "learning!\n",
      "Episode 900 | Reward: 27.01 | Moving Average: 199.55\n",
      "learning!\n",
      "Episode 910 | Reward: 281.21 | Moving Average: 229.93\n",
      "Episode 920 | Reward: 120.24 | Moving Average: 264.32\n",
      "learning!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # Create environment (slightly modified version of CarRacing environment)\n",
    "    env = CustomCarRacing()\n",
    "    \n",
    "    # Create and train agent\n",
    "    network = Network(env)\n",
    "    network.train(episodes=2000, ppo = ppo_train, trajectory_size=512, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c55e1d-8b0f-44cd-a8ad-e686f9c10cee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
