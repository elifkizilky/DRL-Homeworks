{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcc5c3b2-5d9a-49d5-8cbd-686be00440a6",
   "metadata": {
    "tags": []
   },
   "source": [
    "DRL HOMEWORK3 GROUP 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97bb8e9c-5374-483d-815e-ea45d473c1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "import os\n",
    "import random\n",
    "\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import uuid\n",
    "import shutil\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f9b78f-ff3f-418b-ac46-041d0a19cc05",
   "metadata": {},
   "source": [
    "TASK 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dfed178f-60b7-477b-85a5-f1c255067046",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Experience Replay Buffer.\n",
    "    When maximum size is reached, it forgets the oldest element, aka first element.\n",
    "'''\n",
    "class ReplayBuffer():\n",
    "\n",
    "  def __init__(self, size = 10000):\n",
    "    self.size = size\n",
    "    self.transitions = []\n",
    "\n",
    "  def add(self, transition):\n",
    "    if len(self.transitions) < self.size: \n",
    "      self.transitions.append(transition)\n",
    "    else:\n",
    "      self.transitions.pop(0)\n",
    "      self.transitions.append(transition)\n",
    "\n",
    "  def length(self):\n",
    "    return len(self.transitions)\n",
    "\n",
    "  def get_batch(self, batch_size):\n",
    "    return random.sample(self.transitions, batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6a09bb-75ab-4dc7-a9a8-e3d3226bf831",
   "metadata": {},
   "source": [
    "TASK 2 & TASK 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "325d819b-00ed-4ddd-a1f2-0dfd9fc7544e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Function for calculating the loss. Takes a batch of state transitions,\n",
    "    uses the true and predicted outputs to calculate the Huber loss.\n",
    "'''\n",
    "def masked_huber_loss(mask_value, clip_delta):\n",
    "  def f(y_true, y_pred):\n",
    "    error = y_true - y_pred\n",
    "    cond  = K.abs(error) < clip_delta\n",
    "    mask_true = K.cast(K.not_equal(y_true, mask_value), K.floatx())\n",
    "    masked_squared_error = 0.5 * K.square(mask_true * (y_true - y_pred))\n",
    "    linear_loss  = mask_true * (clip_delta * K.abs(error) - 0.5 * (clip_delta ** 2))\n",
    "    huber_loss = tf.where(cond, masked_squared_error, linear_loss)\n",
    "    return K.sum(huber_loss) / K.sum(mask_true)\n",
    "  f.__name__ = 'masked_huber_loss'\n",
    "  return f\n",
    "\n",
    "input_shape = (9,) # 8 variables in the environment + the fraction finished we add ourselves \n",
    "#(fraction means how far the episode to the ending bc. of reaching maximum steps. This way, agent can decide how fast it should take action)\n",
    "outputs = 4\n",
    "\n",
    "'''\n",
    "    Function to create a model. This model can be used to predict the Q values. \n",
    "'''\n",
    "def create_model(learning_rate, regularization_factor):\n",
    "  model = Sequential([\n",
    "    Dense(64, input_shape=input_shape, activation=\"relu\", kernel_regularizer=l2(regularization_factor)),\n",
    "    Dense(64, activation=\"relu\", kernel_regularizer=l2(regularization_factor)),\n",
    "    Dense(64, activation=\"relu\", kernel_regularizer=l2(regularization_factor)),\n",
    "    Dense(outputs, activation='linear', kernel_regularizer=l2(regularization_factor))\n",
    "  ])\n",
    "\n",
    "  optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "  model.compile(optimizer=optimizer, loss=masked_huber_loss(0.0, 1.0))\n",
    "  \n",
    "  return model\n",
    "\n",
    "'''\n",
    " Functions for q values. These values are to be used to select the action.\n",
    "'''\n",
    "def get_q_values(model, state):\n",
    "  input = state[np.newaxis, ...]\n",
    "  return model.predict(input)[0]\n",
    "\n",
    "def get_multiple_q_values(model, states):\n",
    "  return model.predict(states)\n",
    "\n",
    "'''\n",
    "    Epsilon greedy approach to select the action to take\n",
    "'''\n",
    "def select_action_epsilon_greedy(q_values, epsilon):\n",
    "  random_value = random.uniform(0, 1)\n",
    "  if random_value < epsilon: \n",
    "    return random.randint(0, len(q_values) - 1)\n",
    "  else:\n",
    "    return np.argmax(q_values)\n",
    "\n",
    "def select_best_action(q_values):\n",
    "  return np.argmax(q_values)\n",
    "\n",
    "\n",
    "'''\n",
    "    State transition\n",
    "'''\n",
    "class StateTransition():\n",
    "\n",
    "  def __init__(self, old_state, action, reward, new_state, done):\n",
    "    self.old_state = old_state\n",
    "    self.action = action\n",
    "    self.reward = reward\n",
    "    self.new_state = new_state\n",
    "    self.done = done\n",
    "\n",
    "'''\n",
    "   Calculating the target values for each state-action pairs using DQN technique\n",
    "'''    \n",
    "def calculate_target_values(model, target_model, state_transitions, discount_factor):\n",
    "  states = []\n",
    "  new_states = []\n",
    "  for transition in state_transitions:\n",
    "    states.append(transition.old_state)\n",
    "    new_states.append(transition.new_state)\n",
    "\n",
    "  new_states = np.array(new_states)\n",
    "\n",
    "  q_values_new_state = get_multiple_q_values(model, new_states)\n",
    "  q_values_new_state_target_model = get_multiple_q_values(target_model, new_states)\n",
    "  \n",
    "  targets = []\n",
    "  for index, state_transition in enumerate(state_transitions):\n",
    "    best_action = select_best_action(q_values_new_state[index])\n",
    "    best_action_next_state_q_value = q_values_new_state_target_model[index][best_action]\n",
    "    \n",
    "    if state_transition.done:\n",
    "      target_value = state_transition.reward\n",
    "    else:\n",
    "      target_value = state_transition.reward + discount_factor * best_action_next_state_q_value\n",
    "\n",
    "    target_vector = [0] * outputs\n",
    "    target_vector[state_transition.action] = target_value\n",
    "    targets.append(target_vector)\n",
    "\n",
    "  return np.array(targets)\n",
    "\n",
    "\n",
    "'''\n",
    "    To train the model\n",
    "'''\n",
    "def train_model(model, states, targets):\n",
    "  model.fit(states, targets, epochs=1, batch_size=len(targets), verbose=0) \n",
    "\n",
    "'''\n",
    "    Copy the model to create and update the target model\n",
    "'''\n",
    "def copy_model(model):\n",
    "  backup_file = 'backup_'+str(uuid.uuid4())\n",
    "  model.save(backup_file)\n",
    "  new_model = load_model(backup_file, custom_objects={ 'masked_huber_loss': masked_huber_loss(0.0, 1.0) })\n",
    "  shutil.rmtree(backup_file)\n",
    "  return new_model\n",
    "\n",
    "\n",
    "'''\n",
    "    Keeps count of the average reward over the last 100 steps.\n",
    "'''\n",
    "class AverageRewardTracker():\n",
    "  current_index = 0\n",
    "\n",
    "  def __init__(self, num_rewards_for_average=100):\n",
    "    self.num_rewards_for_average = num_rewards_for_average\n",
    "    self.last_x_rewards = []\n",
    "\n",
    "  def add(self, reward):\n",
    "    if len(self.last_x_rewards) < self.num_rewards_for_average: \n",
    "      self.last_x_rewards.append(reward)\n",
    "    else:\n",
    "      self.last_x_rewards[self.current_index] = reward\n",
    "      self.__increment_current_index()\n",
    "\n",
    "  def __increment_current_index(self):\n",
    "    self.current_index += 1\n",
    "    if self.current_index >= self.num_rewards_for_average: \n",
    "      self.current_index = 0\n",
    "\n",
    "  def get_average(self):\n",
    "    return np.average(self.last_x_rewards)\n",
    "\n",
    "'''\n",
    "    Writes the results to a file after every episode.\n",
    "    They are to be used to plot a graph\n",
    "'''\n",
    "class FileLogger():\n",
    "\n",
    "  def __init__(self, file_name='progress.log'):\n",
    "    self.file_name = file_name\n",
    "    self.clean_progress_file()\n",
    "\n",
    "  def log(self, episode, steps, reward, average_reward):\n",
    "    f = open(self.file_name, 'a+')\n",
    "    f.write(f\"{episode};{steps};{reward};{average_reward}\\n\")\n",
    "    f.close()\n",
    "\n",
    "  def clean_progress_file(self):\n",
    "    if os.path.exists(self.file_name):\n",
    "      os.remove(self.file_name)\n",
    "    f = open(self.file_name, 'a+')\n",
    "    f.write(\"episode;steps;reward;average\\n\")\n",
    "    f.close()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "23ed4f17-1bb0-4f38-a5e7-262b7ce97478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: backup_a56b4274-ad3d-49e9-82d3-35a873cf2b29\\assets\n",
      "Starting episode 0 with epsilon 1.0\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "Q values: [ 0.30089393  0.08100026 -0.00496094 -0.0164764 ]\n",
      "Max Q: 0.3008939325809479\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17056/3095188796.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_steps\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mstep_count\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m     \u001b[0mq_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_q_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m     \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_action_epsilon_greedy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[0mnew_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17056/1043440474.py\u001b[0m in \u001b[0;36mget_q_values\u001b[1;34m(model, state)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_q_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m   \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m...\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_multiple_q_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\elifk\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\elifk\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   2031\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2032\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2033\u001b[1;33m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2034\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2035\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\elifk\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\elifk\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\elifk\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    952\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 954\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    955\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    956\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32mD:\\Users\\elifk\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2451\u001b[0m       (graph_function,\n\u001b[0;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2453\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2454\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\elifk\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1859\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1860\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1861\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mD:\\Users\\elifk\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    495\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    496\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 497\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    498\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    499\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\elifk\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Defining parameters\n",
    "replay_buffer_size = 200000\n",
    "learning_rate = 0.001\n",
    "regularization_factor = 0.001\n",
    "training_batch_size = 128\n",
    "training_start = 256\n",
    "max_episodes = 10000\n",
    "max_steps = 1000\n",
    "target_network_replace_frequency_steps = 1000\n",
    "model_backup_frequency_episodes = 100\n",
    "starting_epsilon = 1.0\n",
    "minimum_epsilon = 0.01\n",
    "epsilon_decay_factor_per_episode = 0.995\n",
    "discount_factor = 0.99\n",
    "train_every_x_steps = 4\n",
    "\n",
    "\n",
    "replay_buffer = ReplayBuffer(replay_buffer_size)\n",
    "model = create_model(learning_rate, regularization_factor)\n",
    "target_model = copy_model(model)\n",
    "epsilon = starting_epsilon\n",
    "step_count = 0\n",
    "average_reward_tracker = AverageRewardTracker(100)\n",
    "file_logger = FileLogger()\n",
    "\n",
    "for episode in range(max_episodes):\n",
    "  print(f\"Starting episode {episode} with epsilon {epsilon}\")\n",
    "\n",
    "  episode_reward = 0\n",
    "  state = env.reset()\n",
    "  fraction_finished = 0.0\n",
    "  state = np.append(state, fraction_finished)\n",
    "\n",
    "  first_q_values = get_q_values(model, state)\n",
    "  print(f\"Q values: {first_q_values}\")\n",
    "  print(f\"Max Q: {max(first_q_values)}\")\n",
    "\n",
    "  for step in range(1, max_steps + 1):\n",
    "    step_count += 1\n",
    "    q_values = get_q_values(model, state)\n",
    "    action = select_action_epsilon_greedy(q_values, epsilon)\n",
    "    new_state, reward, done, info = env.step(action)\n",
    "    \n",
    "    fraction_finished = (step + 1) / max_steps\n",
    "    new_state = np.append(new_state, fraction_finished)\n",
    "    \n",
    "    episode_reward += reward\n",
    "\n",
    "    if step == max_steps:\n",
    "      print(f\"Episode reached the maximum number of steps. {max_steps}\")\n",
    "      done = True\n",
    "\n",
    "    state_transition = StateTransition(state, action, reward, new_state, done)\n",
    "    replay_buffer.add(state_transition)\n",
    "\n",
    "    state = new_state\n",
    "\n",
    "    if step_count % target_network_replace_frequency_steps == 0:\n",
    "      print(\"Updating target model\")\n",
    "      target_model = copy_model(model)\n",
    "\n",
    "    if replay_buffer.length() >= training_start and step_count % train_every_x_steps == 0:\n",
    "      batch = replay_buffer.get_batch(batch_size=training_batch_size)\n",
    "      targets = calculate_target_values(model, target_model, batch, discount_factor)\n",
    "      states = np.array([state_transition.old_state for state_transition in batch])\n",
    "      train_model(model, states, targets)\n",
    "\n",
    "    if done:\n",
    "      break\n",
    "\n",
    "  average_reward_tracker.add(episode_reward)\n",
    "  average = average_reward_tracker.get_average()\n",
    "\n",
    "  print(\n",
    "    f\"episode {episode} finished in {step} steps with reward {episode_reward}. \"\n",
    "    f\"Average reward over last 100: {average}\")\n",
    "  file_logger.log(episode, step, episode_reward, average)\n",
    "\n",
    "  if episode != 0 and episode % model_backup_frequency_episodes == 0:\n",
    "    backup_file = f\"model_{episode}.h5\"\n",
    "    print(f\"Backing up model to {backup_file}\")\n",
    "    model.save(backup_file)\n",
    "\n",
    "  epsilon *= epsilon_decay_factor_per_episode\n",
    "  epsilon = max(minimum_epsilon, epsilon)\n",
    "    \n",
    "  if average >= 200:\n",
    "    print('\\nEnvironment solved in {:d} episodes!'.format(episode))\n",
    "    break\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fe31db-a255-4f17-916a-ad33f79f4e50",
   "metadata": {},
   "source": [
    "PLOTTING THE GRAPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0adfe29-629f-4476-a915-29eaaaa3f833",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(file_logger.file_name, sep=';')\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(data['average'])\n",
    "plt.plot(data['reward'])\n",
    "plt.title('Reward')\n",
    "plt.ylabel('Reward')\n",
    "plt.xlabel('Episode')\n",
    "plt.legend(['Average reward', 'Reward'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09039212-4e92-4f1b-ae08-f9ecae36d49d",
   "metadata": {},
   "source": [
    "REFERENCES\n",
    "1- https://wingedsheep.com/lunar-lander-dqn/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
